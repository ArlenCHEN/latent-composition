{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c863920f-3f69-4d5f-b0e2-a2271709c525",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from networks import networks\n",
    "from utils import masking, show, renormalize, compositions, imutil\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from copy import deepcopy\n",
    "\n",
    "def tensor2image(im_tensor):\n",
    "    '''\n",
    "    Convert an image batch (size=1) to an image with values being within [0, 1]\n",
    "    im_tensor - shape: [1, 3, h, w]; value range: [-1, 1]\n",
    "    '''\n",
    "    im_array = im_tensor.data[0].detach().cpu().numpy()\n",
    "    im_array = np.transpose(im_array, (1,2,0))\n",
    "    im_array += 1\n",
    "    im_array /=2\n",
    "\n",
    "    return im_array\n",
    "\n",
    "def show_image(im):\n",
    "    plt.imshow(im)\n",
    "    plt.show()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    # Read the fake image\n",
    "    fake_img_path = '/home/uss00067/Datasets/FDC/video_1/angry/level_1/024/000/3/overlaid.jpg'\n",
    "    # Read the gt image\n",
    "    real_img_path = '/home/uss00067/Datasets/FDC/video_1/angry/level_1/024/000/3/015.jpg'\n",
    "    fake_img_bgr = cv2.imread(fake_img_path)\n",
    "    real_img_bgr = cv2.imread(real_img_path)\n",
    "\n",
    "    print('fake img shape: ', fake_img_bgr.shape)\n",
    "    print('real img shape: ', real_img_bgr.shape)\n",
    "    \n",
    "    # Convert the BGR image to RGB image\n",
    "    fake_img_rgb = cv2.cvtColor(fake_img_bgr, cv2.COLOR_BGR2RGB)\n",
    "    real_img_rgb = cv2.cvtColor(real_img_bgr, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    original_fake_img_rgb = deepcopy(fake_img_rgb)\n",
    "    \n",
    "    gtype = 'stylegan'\n",
    "    domain = 'ffhq'\n",
    "    nets = networks.define_nets(gtype, domain)\n",
    "    # proggan: celebahq, livingroom, church\n",
    "    # stylegan: ffhq, church, car, horse\n",
    "    \n",
    "    compositer = compositions.get_compositer(domain)(nets)\n",
    "    \n",
    "    rng = np.random.RandomState(0)\n",
    "    indices = rng.choice(compositer.total_samples, len(compositer.ordered_labels))\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        raise RuntimeError('cuda is not available!')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Composition over original data\n",
    "        composite_data = compositer(indices)\n",
    "        \n",
    "        # with batch_size = 1\n",
    "        input_data = composite_data.composite_image\n",
    "        output_data = composite_data.inverted_RGBM\n",
    "        \n",
    "        # Convert tensor batch to individual image\n",
    "        input_data_image = tensor2image(input_data)\n",
    "        show_image(input_data_image)\n",
    "        \n",
    "        output_data_image = tensor2image(output_data)\n",
    "        show_image(output_data_image)\n",
    "        \n",
    "        # Original mask value\n",
    "        # 0.5: missing pixel region\n",
    "        # 1: contents extraction region\n",
    "        composite_mask = composite_data.composite_mask[0]\n",
    "        composite_mask = composite_mask.unsqueeze(0)\n",
    "        composite_mask_image = tensor2image(composite_mask)\n",
    "        show_image(composite_mask_image)\n",
    "        \n",
    "        # Normalize the image from [0, 255] to [-1, 1]\n",
    "        # All values have to be converted to float before computation\n",
    "        normalize_factor = int(np.max(fake_img_rgb)/2.)\n",
    "        fake_img_rgb = fake_img_rgb.astype(float)\n",
    "        normalize_fake_img_rgb = (fake_img_rgb - normalize_factor)/normalize_factor\n",
    "        fake_img_rgb_temp = np.transpose(normalize_fake_img_rgb, (2,0,1))\n",
    "        \n",
    "        # Convert numpy array to tensor on GPU\n",
    "        fake_img_rgb_tensor = torch.tensor(fake_img_rgb_temp).unsqueeze(0).float()\n",
    "        fake_img_rgb_tensor = fake_img_rgb_tensor.to(device)\n",
    "        \n",
    "        # Generate mask with all values being 1 (no 0.5: no missing pixels)\n",
    "        fake_img_mask = np.ones((fake_img_rgb.shape[0], fake_img_rgb.shape[1]))\n",
    "        fake_img_mask_tensor = torch.tensor(fake_img_mask).unsqueeze(0)\n",
    "        fake_img_mask_tensor = fake_img_mask_tensor.unsqueeze(0).float()\n",
    "        fake_img_mask_tensor = fake_img_mask_tensor.to(device)\n",
    "        \n",
    "        # Reconstruct the image based on the fake image tensor and fake mask tensor\n",
    "        # batch size of rec_data: 1\n",
    "        rec_data = compositer.nets_RGBM.invert(fake_img_rgb_tensor, fake_img_mask_tensor)\n",
    "        rec_data_image = tensor2image(rec_data)\n",
    "        show_image(original_fake_img_rgb) # Show the stitched image\n",
    "        show_image(rec_data_image) # Show the reconstructed image\n",
    "        show_image(real_img_rgb) # Show the gt image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inversion",
   "language": "python",
   "name": "inversion"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}